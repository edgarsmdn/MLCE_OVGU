{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise_5_Hybrid_Modelling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSMsxFeTYxmh"
      },
      "source": [
        "# Exercise 5. Serial hybrid model for CSTR\n",
        "\n",
        "Let' say we have the following reactor model:\n",
        "\n",
        "<img src=\"CSTR.png\" alt=\"CSTR\" style=\"width: 200px;\"/>\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\mathrm{d}c_A}{\\mathrm{d}t} &= \\frac{c_{A,in} - c_A}{\\tau} - r\\\\\n",
        "\\frac{\\mathrm{d}c_B}{\\mathrm{d}t} &= \\frac{c_{B,in} - c_B}{\\tau} - r\\\\\n",
        "\\frac{\\mathrm{d}c_X}{\\mathrm{d}t} &= \\frac{c_{X,in} - c_X}{\\tau} + r,\n",
        "\\end{align}\n",
        "\n",
        "where $c_i$ denotes the molar concentration of substance $i$ and $r$ is the reaction rate. We assume that the reactor is ideally mixed, that it has a constant volume and that the average residence time is $\\tau=100\\,\\mathrm{1/s}$. The inlet concentrations of substance $i$ into the reactor are given by\n",
        "\n",
        "\\begin{align}\n",
        "c_{A,in} &= 0.7\\,\\mathrm{kmol/m^3}\\\\\n",
        "c_{B,in} &= 0.3\\,\\mathrm{kmol/m^3}\\\\\n",
        "c_{X,in} &= 0\\,\\mathrm{kmol/m^3}.\n",
        "\\end{align}\n",
        "\n",
        "For solving the differential equation system, we also need to know the initial concentration of each substance inside the reactor, denoted as $c_{i,0}$. From the data it is known that the experiment was started with the following initial concentrations:\n",
        "\n",
        "\\begin{align}\n",
        "c_{A,0} &= 0.5\\,\\mathrm{kmol/m^3}\\\\\n",
        "c_{B,0} &= 0.5\\,\\mathrm{kmol/m^3}\\\\\n",
        "c_{X,0} &= 0\\,\\mathrm{kmol/m^3}.\n",
        "\\end{align}\n",
        "\n",
        "Time-dependent measurements for all concentrations are available.\n",
        "\n",
        "The task is to construct a serial hybrid model, in which the reaction rate $r$ is described by a data-driven model. In this particular example, a neural network will be used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB3Ws3TGYxmp"
      },
      "source": [
        "# Problem description\n",
        "\n",
        "The problem with constructing a serial hybrid model, as exemplified above, is that the data-driven model cannot be trained independently of the differential equation system that describes the system (mechanistic part of the hybrid model). Actually, the neural network should learn the function $r=f(c)$ but, of course, we do not have values for $r$ which we could use to train the model. One way to solve this problem is by using sensitivity equations. This approach will be discussed in the following."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwkyCdLAYxmq"
      },
      "source": [
        "# General solution procedure\n",
        "\n",
        "We will first generalize the differential equation system as follows:\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\mathrm{d}y}{\\mathrm{d}t} &= f(y, u,\\phi(y,w))\n",
        "\\end{align}\n",
        "\n",
        "where $y$ are states of the system, $u$ are system inputs and $\\phi(y,w)$ denotes a data-driven model that describes a part of the mechanistic model with the use of the states $y$ and a set of parameter values $w$. In the case of a neural network, the parameter vector $w$ includes all the weights and biases.\n",
        "\n",
        "In order to train any kind of model, we need a loss or cost function (usually denoted by $J$) to estimate the quality of our model predictions. Often times, the sum of squares of the deviations between the model predictions and the data is used as an objective for parameter estimation:\n",
        "\n",
        "\\begin{align}\n",
        "J = 0.5 \\sum_{i=1}^N\\left(y_i - y_{i, \\text{exp}}\\right)^2,\n",
        "\\end{align}\n",
        "\n",
        "where $y$ are the set of model predictions corresponding to the $N$ measurement points $y_\\text{exp}$. The model predictions are depend on $u$ and $w$, i.e. $y=f(u,w)$.\n",
        "\n",
        "Normally, when training a neural network, the gradient of the loss function with respect to the network parameters is used for optimization. The gradient of the loss function with respect to a single parameter $w_j$ is given by\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial J}{\\partial w_j} = \\sum_{i=1}^N\\left(y_i - y_{i, \\text{exp}}\\right)\\frac{\\partial y}{\\partial w_j}.\n",
        "\\end{align}\n",
        "\n",
        "As can be seen, the gradient depends on $\\frac{\\partial y}{\\partial w_j}$, i.e. the sensitivities of the system states with respect to the parameters (neural network weights and biases). Training of the neural network is then achieved by iteratively updating the parameters according to\n",
        "\n",
        "\\begin{align}\n",
        "w_j^{n+1} = w_j^{n} - g \\frac{\\partial J}{\\partial w_j^{n}},\n",
        "\\end{align}\n",
        "\n",
        "where $w_j^{n+1}$ is the updated parameter that is calculated from the current parameter $w_j^{n}$ using the gradient $\\frac{\\partial J}{\\partial w_j^{n}}$ and a learning rate $g$.\n",
        "\n",
        "As can be seen, the problem could be solved easily, if we would have the sensitivities $\\frac{\\partial y}{\\partial w_j}$. Since the mechanistic part of the model is given by a differential equation system, calculating these sensitivities is a bit more complicated than for an algebraic model, but luckily they are available. Using local sensitivity analysis of the ODE system and denoting the sensitivities $\\frac{\\partial y}{\\partial w_j}$ as $s_j$, the sensitivities can be described by\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\mathrm{d}s_j}{\\mathrm{d}t} = \\frac{\\partial f}{\\partial y}s_j + \\frac{\\partial f}{\\partial w_j}.\n",
        "\\end{align}\n",
        "\n",
        "As can be seen, the equation above is a differential equation for the sensitivities. Since $f$ depends on the system states, the differential equations for the sensitivities need to be integrated simultaneously with the ODE system of the mechanistic part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz6WCB3hYxmr"
      },
      "source": [
        "# Solution to the example\n",
        "\n",
        "For the reactor example above, the loss function is defined as\n",
        "\n",
        "\\begin{align}\n",
        "J = 0.5\\left[ \\sum_{i=1}^N\\left(c_{A,i} - c_{A,i,\\text{exp}}\\right)^2 + \\sum_{i=1}^N\\left(c_{B,i} - c_{B,i,\\text{exp}}\\right)^2 + \\sum_{i=1}^N\\left(c_{X,i} - c_{X,i,\\text{exp}}\\right)^2\\right],\n",
        "\\end{align}\n",
        "\n",
        "if we assume that we have measurements for all components at each point $i$. Then, the gradient of the loss function with respect to the parameters $w$ is given as \n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial J}{\\partial w_j} = \\sum_{i=1}^N\\left(c_{A,i} - c_{A,i,\\text{exp}}\\right)\\frac{\\partial c_{A,i}}{\\partial w_j} + \\sum_{i=1}^N\\left(c_{B,i} - c_{B,i,\\text{exp}}\\right)\\frac{\\partial c_{B,i}}{\\partial w_j} + \\sum_{i=1}^N\\left(c_{X,i} - c_{X,i,\\text{exp}}\\right)\\frac{\\partial c_{X,i}}{\\partial w_j},\n",
        "\\end{align}\n",
        "\n",
        "since the concentrations $c_A$, $c_B$ and $c_X$ are all functions of the parameters of the neural network (weights and biases). The sensitivities are calculated according to the following ODE system for each parameter $w_j$:\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\mathrm{d}}{\\mathrm{d}t}\\begin{bmatrix}\n",
        "  \\frac{\\partial c_{A}}{\\partial w_j}\\\\\n",
        "  \\frac{\\partial c_{B}}{\\partial w_j}\\\\\n",
        "  \\frac{\\partial c_{X}}{\\partial w_j}\\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "  -\\frac{1}{\\tau}-\\frac{\\partial r}{\\partial c_A} & -\\frac{\\partial r}{\\partial c_B} & -\\frac{\\partial r}{\\partial c_X}\\\\\n",
        "  -\\frac{\\partial r}{\\partial c_A} & -\\frac{1}{\\tau}-\\frac{\\partial r}{\\partial c_B} & -\\frac{\\partial r}{\\partial c_X} \\\\\n",
        "  +\\frac{\\partial r}{\\partial c_A} & +\\frac{\\partial r}{\\partial c_B} & -\\frac{1}{\\tau}+\\frac{\\partial r}{\\partial c_X} \\\\\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "  \\frac{\\partial c_{A}}{\\partial w_j}\\\\\n",
        "  \\frac{\\partial c_{B}}{\\partial w_j}\\\\\n",
        "  \\frac{\\partial c_{X}}{\\partial w_j}\\\\\n",
        "\\end{bmatrix}\n",
        "+\n",
        "\\begin{bmatrix}\n",
        "  -\\frac{\\partial r}{\\partial w_j}\\\\\n",
        "  -\\frac{\\partial r}{\\partial w_j}\\\\\n",
        "  +\\frac{\\partial r}{\\partial w_j}\\\\\n",
        "\\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "Hence, for $p$ parameters and $l$ system states, there are $p*l$ sensitivity equations to be integrated additionally to the $l$ system equations. If a neural network with 3 input nodes, 10 hidden nodes and 1 output node is used, the total number of parameters is $3*10+10+10*1+1=51$ parameters for three states ($c_A$, $c_B$ and $c_X$). Thus, there would be $3*51=153$ sensitivity equations to be integrated, which quickly becomes computationally intensive."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# autograd is used for exact, automatic differentiation\n",
        "!pip install autograd\n",
        "import autograd.numpy as np\n",
        "from autograd.scipy.integrate import odeint\n",
        "import autograd.numpy.random as npr\n",
        "from autograd import jacobian\n",
        "from autograd.misc.optimizers import adam\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "uRnmcH8FfqkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "###           Create neural network for reaction rate prediction             ###\n",
        "################################################################################\n",
        "\n",
        "# initialize random number generator for repeatable results\n",
        "rs = npr.RandomState(0)\n",
        "\n",
        "def init_random_params(layer_sizes, scale):\n",
        "    \"\"\"Build a list of weights and biases, one for each layer in the net.\n",
        "    layers is a list with the number of nodes in each layer. Minimum number of layers is three (input, hidden \n",
        "    and output) scale is a constant factor to scale the random values (down or up) if necessary\"\"\"\n",
        "    params = []\n",
        "    for idx in range(len(layer_sizes)-1):\n",
        "        weight_mat_elem = layer_sizes[idx]*layer_sizes[idx+1]\n",
        "        bias_vec_elem = layer_sizes[idx+1]\n",
        "        params = np.append(params, rs.randn(weight_mat_elem+bias_vec_elem))\n",
        "    return params*scale\n"
      ],
      "metadata": {
        "id": "SuiF9j8bgH8j"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets take some random values of some variables to understand the forward pass of neural network\n",
        "layer_sizes =[3,3,1]\n",
        "scale = 0.0005\n",
        "init_params = init_random_params(layer_sizes, scale)"
      ],
      "metadata": {
        "id": "x_pICrrCdPzq"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Jjz2pdTaVnE2"
      },
      "outputs": [],
      "source": [
        "def neural_net_predict(params, inputs):\n",
        "    \"\"\"Implements a (deep) neural network for regression.\n",
        "       params is a list of weights and biases.\n",
        "       inputs is a matrix of input values.\n",
        "       returns network prediction.\"\"\"\n",
        "    # Make sure that params is a vector\n",
        "    params = params.flatten()\n",
        "    # set separator value for easier indexing of the parameters and assigning them to weights and biases \n",
        "    # for each layer\n",
        "    sep = 0\n",
        "    # loop over all layers\n",
        "    for idx in range(len(layer_sizes)-1):\n",
        "        # calculate weight matrix\n",
        "        W = params[sep:sep+layer_sizes[idx]*layer_sizes[idx+1]].reshape(layer_sizes[idx],layer_sizes[idx+1])\n",
        "        # calculate bias vector\n",
        "        b = params[sep+layer_sizes[idx]*layer_sizes[idx+1]:sep+layer_sizes[idx]*layer_sizes[idx+1]\n",
        "                   +layer_sizes[idx+1]]\n",
        "        # set new separator value\n",
        "        sep = layer_sizes[idx]*layer_sizes[idx+1]+layer_sizes[idx+1]\n",
        "        # calculate output as weighted sum of inputs plus bias\n",
        "        outputs = np.dot(inputs, W) + b\n",
        "        # apply activation function and assign the result as the input to the next layer \n",
        "        # (note that this has no effect on the output layer)\n",
        "        inputs = 1/(1 + np.exp(-outputs))\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initial conditions for the concentrations in the reactor\n",
        "c_A0 = 0.5\n",
        "c_B0 = 0.5\n",
        "c_X0 = 0\n",
        "c0 = [c_A0, c_B0, c_X0]\n",
        "outputs = neural_net_predict(init_params, c0)\n",
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t67s2IAQfJt3",
        "outputId": "7b9592bc-69b6-4e46-be2a-564137a9977f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.00049865])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "###                 Generate complete serial hybrid model                    ###\n",
        "################################################################################\n",
        "\n",
        "# system parameters\n",
        "tau = 100\n",
        "\n",
        "# inlet concentrations\n",
        "c_Ain = 0.7\n",
        "c_Bin = 0.3\n",
        "c_Xin = 0\n",
        "\n",
        "# initial conditions for the concentrations in the reactor\n",
        "c_A0 = 0.5\n",
        "c_B0 = 0.5\n",
        "c_X0 = 0\n",
        "\n",
        "# end time for integration\n",
        "t_end = 100\n",
        "n_samples = 30\n",
        "t_span = np.linspace(0, t_end, n_samples)\n",
        "t_span"
      ],
      "metadata": {
        "id": "fymDeYGEgQx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define system equations\n",
        "def dcdt(c, t, params):\n",
        "    \"\"\"Mechanistic part of the hybrid model (ODE system describing the time-dependent \n",
        "    concentrations in the reactor)\"\"\"\n",
        "    # disassemble input vector\n",
        "    c_A, c_B, c_X = c\n",
        "    # calculate reaction rates by neural network prediction\n",
        "    r = neural_net_predict(params, c)\n",
        "    #r = 0.08*c_A**0.7*c_B**1.3 # true underlying reaction rate\n",
        "    # system equations\n",
        "    dcdt = [(c_Ain-c_A)/tau - r,\n",
        "            (c_Bin-c_B)/tau - r,\n",
        "            (c_Xin-c_X)/tau + r]\n",
        "    return np.array(dcdt)"
      ],
      "metadata": {
        "id": "wFLvdCTdjFsk"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate system jacobian and parameter derivatives by automatic differentiation with autograd\n",
        "dfdc = jacobian(dcdt, 0)    # system jacobian\n",
        "dfdp = jacobian(dcdt, 2)    # parameter derivatives"
      ],
      "metadata": {
        "id": "uMenTWFCgXEd"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# differential equation system\n",
        "def DiffEqs(y, t, params):\n",
        "    \"\"\"Hybrid model including the ODE system for the concentrations as well as the sensitivities \n",
        "    that are used for training the neural network part of the model\"\"\"\n",
        "    # disassemble input vector\n",
        "    c = y[:3]\n",
        "    s = y[3:]\n",
        "    # evaluate system jacobian at current point\n",
        "    dfdc_eval = dfdc(c, t, params)\n",
        "    # evaluate parameter derivatives at current point\n",
        "    dfdp_eval = dfdp(c, t, params) # Shape: (3, 1, 16)\n",
        "    #print(dfdp_eval.shape)\n",
        "    # define sensitivities for all parameters\n",
        "    dsdt = np.zeros(len(s)) # preallocate memory for sensitivities\n",
        "    for i in range(len_p):  # loop over all parameters to construct the corresponding sensitivity equations\n",
        "        dsdt[i*len_c:(i+1)*len_c] = (dfdc_eval @ s[i*len_c:(i+1)*len_c]).flatten() + dfdp_eval[:,0,i] \n",
        "        # construct sensitivities (see https://docs.sciml.ai/v4.0/analysis/sensitivity.html#Example-solving-an-\n",
        "        # ODELocalSensitivityProblem-1)\n",
        "        # [c1/w1, c2/w1, c3/w1, c1/w2, ...]\n",
        "    return np.concatenate((dcdt(c, t, params).flatten(), dsdt))"
      ],
      "metadata": {
        "id": "36nAnk9CgdkE"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "scale = 0.0005\n",
        "num_epochs = 1000\n",
        "step_size = 0.001\n",
        "\n",
        "# set neural network size\n",
        "layer_sizes = [3, 3, 1] # no. of nodes in input layer, hidden layer(s) and output layer\n",
        "\n",
        "# initialize parameter vector for neural network or load saved parameters\n",
        "init_params = init_random_params(layer_sizes, scale)"
      ],
      "metadata": {
        "id": "CfOxtugKglVm"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assemble initial value vector\n",
        "c0 = [c_A0, c_B0, c_X0]\n",
        "len_c = len(c0)\n",
        "len_p = len(init_params)\n",
        "s0 = np.zeros((len_p*len_c))\n",
        "y0 = np.concatenate((c0,s0))\n",
        "\n",
        "# load some simulated(fake) experimental data (see above)\n",
        "c_exp = np.array(pd.read_csv('/content/ODE-data.txt', sep=';'))"
      ],
      "metadata": {
        "id": "7qpT8g6Tg-iV"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(params, iter):\n",
        "    \"\"\"Objective function (sum of squared errors between measurements and model predictions)\"\"\"\n",
        "    # calculate hybrid model in forward direction with odeint\n",
        "    sol = odeint(DiffEqs, y0, t_span, args=(params,))\n",
        "    # disassemble results\n",
        "    c_pred = sol[:,:3] # predicted concentrations\n",
        "    return np.trace((c_pred - c_exp).T @ (c_pred - c_exp))"
      ],
      "metadata": {
        "id": "-CBjYSzWhFPr"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_grad(params, iter):\n",
        "    \"\"\"Function calculates the gradient of the objective function with respect to the network parameters\"\"\"\n",
        "    sol = odeint(DiffEqs, y0, t_span, args=(params,))\n",
        "    # disassemble results\n",
        "    c_pred = sol[:,:3] # predicted concentrations\n",
        "    sens = sol[:,3:]   # sensititvities 16*3=48 -> c1/w1, c2/w1, c3/w1, c1/w2.....\n",
        "    # calculate gradients of the loss function\n",
        "    loss_grad = np.zeros(len_p) # set vector size\n",
        "    for comp_idx in range(len_c):\n",
        "        # For loop is running for each concentration and all parameters\n",
        "        loss_grad += sens[:,comp_idx::3].T @ (c_pred[:,comp_idx] - c_exp[:,comp_idx])      # : is for all 30 time steps\n",
        "    return loss_grad"
      ],
      "metadata": {
        "id": "3H6KjJfsiuMO"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "9MvPw2uiYxmt"
      },
      "outputs": [],
      "source": [
        "def summary(params, iter, gradient):\n",
        "    \"\"\"Callback function gives informative output during optimization\"\"\"\n",
        "    if iter % 10 == 0:\n",
        "        print('step {0:5d}: {1:1.3e}'.format(iter, objective(params, iter)))\n",
        "        np.save('Params', params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimize the network parameters\n",
        "optimized_params = adam(objective_grad, init_params, step_size=step_size, num_iters=num_epochs, callback=summary)"
      ],
      "metadata": {
        "id": "-w7FV2Ahko7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_params"
      ],
      "metadata": {
        "id": "ij5RhjX907mn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}